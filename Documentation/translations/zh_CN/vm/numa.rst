.. SPDX-License-Identifier: GPL-2.0

.. include:: ../disclaimer-zh_CN.rst

.. _numa:

:Original: Documentation/vm/numa.rst

:译者:

 蒋子奇 Jiang Ziqi <jiangziqi@360.cn>

Started Nov 1999 by Kanoj Sarcar <kanoj@sgi.com>

===========
什么是NUMA？
===========

这个问题可以从几个视角来回答：硬件视角和Linux软件视角。

从硬件视角来看，NUMA系统是一个由多个组件或配件所组成的计算机平台，
每个组件包含0个或多个CPU，本地内存，和本地IO总线。为了简洁和消除
这些物理组件/配件的硬件视图的歧义，根据软件的抽象，我们将在本文档
中将组件/配件称为“cells”。

每个'cell'可以被视为系统的一个SMP[对称多处理器]子集--尽管独立SMP
系统所需的一些组件可能不会在任何给定cell上填充。NUMA系统的cells之
间通过某种系统互联的方式相互连接--比如交叉连接或者点对点连接是通
用的NUMA系统互联的方式。可以聚合使用这两种互联方式来创建NUMA平台，
其中的cells之间有多种不同的距离。

对于Linux来说，我们所感兴趣的是一致性缓存NUMA或者ccNUMA系统。
在ccNUMA系统中，所有内存都是可见的，并且可以绑定在任何cell上的任何
CPU访问，缓存一致性在硬件中由处理器缓存或系统互连来处理。

内存访问时间和有效内存带宽取决于包含进行内存访问的CPU或IO总线的cell
与包含目标内存的cell之间的距离。例如，与访问其他远程cell上的内存相比，
连接到同一cell上的cpu访问内存将经历更快的访问时间和更高的带宽。NUMA
平台可以从给定的cell拥有多个远程cell。

平台供应商构建NUMA系统并不只是为了让软件开发人员的生活更有趣。相反，
这个体系结构是为了提供可拓展内存带宽的一种方法。然而，为了实现可拓展
的内存带宽，系统和应用程序软件必须将大量的内存引用[缓存丢失]分配到
“本地”内存--同一cell上的内存(如果有的话)--或与内存最近的cell。

这就产生了NUMA系统的Linux软件视角：

Linux将系统的硬件资源划分为多个称为“node”的软件抽象。Linux将node映射
到硬件平台的物理cells上，抽象出一些体系结构的细节。与物理cells一样，软
件node可能包含0个或多个cpu、内存或IO总线。同样，内存访问“更近”node上
的内存--映射到更近的cells的node--通常会比访问更远的cells经历更快的访问
时间和更高的有效带宽。

对于某些体系架构(如x86)， Linux将“隐藏”表示没有绑定内存的物理cell的任何
node，并将绑定到该cell的任何cpu重新分配给表示具有内存的单元的node。因此，
在这些体系结构上，不能假设Linux与给定node关联的所有cpu都将看到相同的本地
内存访问时间和带宽。

此外，对于某些体系结构(如x86)，Linux支持对额外node的模拟。对于NUMA仿真，
linux将把现有node(或非NUMA平台的系统内存)分割为多个node。每个模拟node将
管理底层cell的一部分物理内存。NUMA仿真对于在非NUMA平台上测试NUMA内核和
应用程序特性很有用，并且在与cpu一起使用时作为一种内存资源管理机制。
[参考 Documentation/admin-guide/cgroup-v1/cpusets.rst]

对于每个具有内存的node，Linux构建了一个独立的内存管理子系统，包含自己的
空闲页面列表、正在使用的页面列表、使用统计数据和用于控制访问的锁。此外，
Linux为每个内存区[一个或多个DMA, DMA32, NORMAL, HIGH_MEMORY, MOVABLE]，
构造一个有序的“zonelist”。zonelist指定当所选分区/node不能满足分配请求时
要访问的分区/node。当一个区域没有可用内存来满足一个请求时，这种情况称为
“溢出”或“回退”。

因为一些node包含多个包含不同类型内存的区域，所以Linux必须决定是对分区列
表进行排序，使分配回退到不同node上的相同区域类型，还是回退到相同node上的
不同区域类型。这是一个重要的考虑因素，因为一些区域(如DMA或DMA32)表示相对
稀缺的资源。Linux选择默认的node有序分区列表。这意味着在使用按NUMA距离排序
的远程node之前，它尝试从同一node回退到其他区域。

默认情况下，Linux将尝试满足内存分配请求，从执行分配请求的cpu所在的node的
内存中分配。具体来说，Linux将尝试从适当的分区列表中的第一个node为发起请求
的node进行分配。这叫做“本地分配”。如果“本地”node不能满足请求，内核将检查
选中的zonelist中其他node的zone，寻找列表中第一个可以满足请求的分区。Linux
将尝试满足内存分配请求，从执行分配请求的cpu所在的node的内存中分配。

本地分配倾向于将对已分配内存的后续访问保持在“本地”底层物理资源，并避免系统
互连 -- 只要内核为任务分配了一些内存以后不会从该内存迁移出去。Linux调度器
知道平台的NUMA拓扑 -- 具体体现在“调度域”数据结构中[参考 Documentation/scheduler/sched-domains.rst]
-- 调度器尝试最小化任务迁移到远程调度域。但是，调度器不直接考虑任务的NUMA
占用。因此，在非常不平衡的情况下，任务可以在节点之间迁移，离开它们的初始节
点和内核数据结构。

系统管理员和应用程序设计人员可以使用各种CPU亲和性命令行接口(如taskset(1)
和numactl(1))以及程序接口(如sched_setaffinity(2))来限制任务的迁移以改善NUMA
本地性。此外，还可以使用Linux NUMA内存策略修改内核的默认本地分配行为。[参考
:ref:`Documentation/admin-guide/mm/numa_memory_policy.rst <numa_memory_policy>`].

系统管理员可以使用控制组和cpu集，限制非特权用户可以在调度或NUMA命令和函数中，
指定cpu和node内存。[参考 Documentation/admin-guide/cgroup-v1/cpusets.rst]

在不隐藏无内存node的体系架构上，Linux将只在分区列表中包含有内存的区域[node]。
这意味着对于一个无内存node，“本地内存node”--CPU node的zonelist中的第一个区域
的node--将不是node本身。相反，它将是内核在构建zonelist时选择的具有内存的最近
node。因此，默认情况下，本地分配将成功，内核将提供最近的可用内存。这是同一种
机制的结果，这种机制允许当一个node包含内存溢出时，这种分配回退到附近的其他node。

一些内核分配不希望或不能容忍这种分配回退行为。相反，它们希望确保从指定的node
获得内存，或者得到node没有空闲内存的通知。例如，当子系统分配每个CPU内存资源时，
通常会出现这种情况。

进行这种分配的典型模型是使用内核的numa_node_id()或CPU_to_node()函数获取“当前CPU”
所绑定的node的node id，然后仅从返回的node id请求内存。当这样的分配失败时，请求
子系统可能会恢复到它自己的回退路径。slab内核内存分配器就是一个例子。或者，子系
统可以选择在分配失败时禁用或不启用自己。内核分析子系统就是一个例子。

如果体系结构支持--而不是隐藏--无内存node，那么连接到无内存node的cpu总是会导致
回退路径开销，或者如果一些子系统试图从一个没有内存的node独占地分配内存，那么
它们将无法初始化。为了透明地支持这样的架构，内核子系统可以使用numa_mem_id()
或cpu_to_mem()函数来为调用或指定的CPU定位“本地内存node”。同样，这是将尝试从默认
情况下进行本地页面分配的同一node。
